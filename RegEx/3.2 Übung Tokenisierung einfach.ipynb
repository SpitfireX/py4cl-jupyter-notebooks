{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementierung eines einfachen Tokenizers\n",
    "\n",
    "Ziel dieser Übung ist es, diverse Texte einzulesen, diese zu tokenisieren und als Folge von Tokens auszugeben. Hierbei liegt der Fokus nicht auf Vollständigkeit! Die Eingabetexte müssen nicht *perfekt* tokenisiert werden, nur so gut wie möglich verarbeitet werden.\n",
    "\n",
    "## 1. Einlesen und Vorverarbeitung der Texte\n",
    "\n",
    "Im Ordner `tokenizer_data` befinden sich mehrere Dateien mit Eingabetexten in drei verschiedenen Eingabeformaten. Die ersten Schritte sind nun, diese Dateien einzulesen, die Texte zu extrahieren und von unerwünschten Elementen zu säubern, sowie die reinen Texte gesammelt in eine dafür geeignete Datenstruktur zu legen.\n",
    "\n",
    "`cmc_train_whats_app.txt` und `cmc_train_wiki_discussion_2.txt` sind einfache Textdateien, die durch Leerzeilen getrennte Einzeltexte enthalten. `cmc_train_whats_app.txt` enthält einzelne WhatsApp-Nachrichten. `cmc_train_wiki_discussion_2.txt` enthält einzelne Beiträge von Wikipedia-Diskussionsseiten. Jeder Text hat dabei eine Metadatenkopfzeile in einem XML-ähnlichen Format. Diese Kopfzeile soll entfernt werden, und die einzelnen Texte sollen getrennt bereitgestellt werden.\n",
    "\n",
    "`epub_alice1.xml` und `epub_mittelpunkt3.xml` enthalten jeweils ein einzelnes Kapitel aus einer E-Book-Ausgabe von Lewis Carrolls \"Alice's Adventures in Wonderland\" und Jules Vernes \"Reise nach dem Mittelpunkt der Erde\". Die Kapiteldateien selbst liegen als XHTML-Dokumente vor und stammen so unverändert aus den ursprünglichen Epub-Dateien, aus denen sie extrahiert wurden. Bei diesen Dateien sollen \n",
    "1. alle Absätze des eigentlichen Textes extrahiert werden,\n",
    "2. der Text der Absätze von allen XML-Tags und Zeilenumbrüchen befreit werden,\n",
    "3. die einzelnen Absätze durch Zeilenumbrüche getrennt zusammengefügt werden.\n",
    "\n",
    "`twitter-corpus.csv` enthält Tweets als CSV-Tabelle. Hier sollen aus der Spalte \"TweetText\" die einzelnen Tweets als Texte bereitgestellt werden. Zum Einlesen der CSV-Datei kann entweder wieder Pandas mit der Funktion `read_table()` oder das `csv`-Modul aus der Python-Standardbibliothek verwendet werden (siehe Übung 3.1).\n",
    "\n",
    "Es bietet sich an, die einzelnen Texte in einer Liste oder einem Dictionary abzuspeichern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import re\n",
    "\n",
    "\"\"\"Liste an Dateinamen der einzulesenden Dateien\"\"\"\n",
    "filenames = [\n",
    "    'tokenizer_data/cmc_train_whats_app.txt',\n",
    "    'tokenizer_data/cmc_train_wiki_discussion_2.txt',\n",
    "    'tokenizer_data/epub_alice1.xml',\n",
    "    'tokenizer_data/epub_mittelpunkt3.xml',\n",
    "    'tokenizer_data/twitter-corpus.csv', \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Diese Funktion liest die Texte aus den cmc_*.txt-Dateien ein und gibt sie als Liste zurück\"\"\"\n",
    "def extract_txt(filename):\n",
    "    texts = []\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Diese Funktion liest die Texte aus den epub_*.xml-Dateien ein und gibt sie als Liste zurück\"\"\"\n",
    "def extract_epub(filename): \n",
    "    texts = []\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Diese Funktion liest die Texte aus der twitter-corpus.csv-Datei ein und gibt sie als Liste zurück\"\"\"\n",
    "def extract_twitter(filename):\n",
    "    texts = []\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hier Code vervollständigen\n",
    "\n",
    "# Hier sollen die Dateien eingelesen und die einzelnen Texte in eine geeignete Datenstruktur abgelegt werden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tokenisierung der Texte\n",
    "\n",
    "Die im vorherigen Schritt gewonnenen Texte sollen nun tokenisiert werden. Dazu müssen anhand des untenstehenden Code-Gerüsts die regulären Ausdrücke bzw. Token-Klassen zum Tokenisieren vervollständigt werden.\n",
    "\n",
    "Eine Token-Klasse bezieht sich auf eine Art von Eingabe, die gesondert behandelt werden muss/kann. Typische Beispiele für gesonderte Token-Klassen sind bspw. Zahlen, URLs und \"normale\" Wörter. \n",
    "Die Token-Klassen und regulären Ausdrücke liegen dabei in einer Hierarchie, da sie der Reihe nach durchlaufen werden. Es ist wichtig, dass dabei die speziellen Token-Klassen in der Hierarchie über den generischen stehen, sprich als erstes angegeben werden. Dadurch, dass in jedem Tokenisierungsschritt die gefundenen Tokens aus dem Eingabetext \"konsumiert\" werden, könnten sonst generischere Token-Klassen den speziellen den Input \"wegnehmen\".\n",
    "Verdeutlicht am Beispieltext und den Token-Klassen aus der Angabe:\n",
    "```python\n",
    "token_classes = {\n",
    "    'emoasc': r'[:;=][-^]?[DP()c|\\[\\]{}]|XD+',\n",
    "    'punct': r'[,.:;()]',\n",
    "    'word': r'\\w+'\n",
    "}\n",
    "```\n",
    "Wenn nun der Eingabetext `Das ist ein Test. :)` tokenisiert wird, wird in der gegebenen Hierarchie als erstes nach Smileys gesucht, dann nach Satzzeichen, dann nach allen anderen zusammenhängenden Wörtern. In diesem Fall stellt \"emoasc\" eine speziellere Klasse für \"gewisse Kombinationen von Satzzeichen\" dar, während \"punct\" eine allgemeine Klasse für \"alle Satzzeichen\" darstellt. Würde man die Klassen \"emoasc\" und \"punct\" vertauschen, könnten \":\" und \")\" als einzelne Satzzeichen im Verarbeitungsschritt von \"punct\" interpretiert werden und nie das Erkennungsmuster für \"emoasc\" erreichen!\n",
    "\n",
    "Welche besonderen und interessanten Tokenklassen gibt es in den Eingabetexten? Gibt es Arten von Tokens, die spezifisch für die Art des Eingabetextes sind? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Token():\n",
    "    \"\"\"Ein Objekt, das ein Token repräsentiert.\n",
    "    Jedes Token hat einen `text` und eine `token_class`, welche die Klasse (also Art) des Tokens angibt. Außerdem werden\n",
    "    seine Start- und Endposition im ursprünglichen Text gespeichert\"\"\"\n",
    "    \n",
    "    def __init__(self, text, token_class, start, end):\n",
    "        self.text = text\n",
    "        self.token_class = token_class\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        \n",
    "    def __str__(self):\n",
    "        return self.text\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'Token(\"{self.text}\", \"{t.token_class}\", {t.start}, {t.end})'\n",
    "    \n",
    "def debug(text):\n",
    "    \"\"\"Diese Funktion tokenisiert den gegebenen Text mit `tokenize()` und gibt zu jedem Token alle Informationen aus\"\"\"\n",
    "    tokens = tokenize(text)\n",
    "    for t in tokens:\n",
    "        print(f'\"{t.text}\":\\tclass=\"{t.token_class}\", start=\"{t.start}\", end=\"{t.end}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"Diese Funktion tokenisiert den in `text` gegebenen Text und gibt eine Liste an Tokens zurück.\n",
    "    \n",
    "    Zum Tokenisieren des Eingabetextes werden die regulären Ausdrücke in `token_classes` der Reihe nach mit `re.finditer()`\n",
    "    auf den Text angewandt. Für jedes Match wird ein neues Token-Objekt erstellt und in einer Liste abgespeichert.\n",
    "    Alle Matches der Regex werden mithilfe von `re.sub()` aus dem Eingabetext entfernt, bevor die nächste Regex in der\n",
    "    Hierarchie aufgerufen wird.\n",
    "    \n",
    "    Diese Funktion muss prinzipiell nicht geändert werden.\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "\n",
    "    for token_class, regex in token_classes.items():\n",
    "        for match in re.finditer(regex, text):\n",
    "            tokens.append(\n",
    "                Token(match[0], token_class, match.start(), match.end())\n",
    "            )\n",
    "\n",
    "        text = re.sub(regex, ' ', text)\n",
    "    \n",
    "    # Die Tokens sind noch nicht in der richtigen Reihenfolge und müssen erst nach ihrer Startposition sortiert werden\n",
    "    return sorted(tokens, key = lambda token: token.start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hier Code vervollständigen ###\n",
    "\n",
    "\"\"\"Die einzelnen regulären Ausdrücke zum Tokenisieren der einzelnen Token-Klassen.\n",
    "Die Schlüssel des Dictionaries sind die Namen der Token-Klassen,\n",
    "die Werte sind die zugehörigen regulären Ausdrücke.\n",
    "Beim Tokenisieren werden die Tokenizer der Reihe nach aufgerufen, beginnend mit dem ersten Element im Dictionary.\"\"\"\n",
    "token_classes = {\n",
    "    'emoasc': r'[:;=][-^]?[DP()c|\\[\\]{}]|XD+',\n",
    "    'punct': r'[,.:;()]',\n",
    "    'word': r'\\w+'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Das\":\tclass=\"word\", start=\"0\", end=\"3\"\n",
      "\"ist\":\tclass=\"word\", start=\"4\", end=\"7\"\n",
      "\"ein\":\tclass=\"word\", start=\"8\", end=\"11\"\n",
      "\"Test\":\tclass=\"word\", start=\"12\", end=\"16\"\n",
      "\".\":\tclass=\"punct\", start=\"16\", end=\"17\"\n",
      "\":)\":\tclass=\"emoasc\", start=\"18\", end=\"20\"\n"
     ]
    }
   ],
   "source": [
    "debug('Das ist ein Test. :)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Ausgabe der Texte\n",
    "\n",
    "Abschließend sollten alle Texte in tokenisierter Form als Datei ausgegeben werden. Ein gängiges Format dafür ist, die Texte als einfache Textdateien auszugeben, wobei immer ein Token pro Zeile steht. D. h.:\n",
    "```\n",
    "Ich\n",
    "habe\n",
    "alle\n",
    "meine\n",
    "Tokens\n",
    "auf\n",
    "separaten\n",
    "Zeilen\n",
    "!\n",
    "=D\n",
    "```\n",
    "Dies kann einfach über die in Python eingebauten Dateioperationen geschehen, sprich z. B. über `file.write()`. Dabei sollte am besten für jede Eingabedatei eine Ausgabedatei entstehen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hier sollen die tokenisierten Texte ausgegeben werden."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
